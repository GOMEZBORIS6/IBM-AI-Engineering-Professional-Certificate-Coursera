{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770ff1d31de447c6b2cb1fa661b82148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66196a23aa245af97a7c0867a2c8912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 13:08:59.046086: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2025-07-29 13:08:59.055663: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394320000 Hz\n",
      "2025-07-29 13:08:59.056676: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57ea4d3c4570 executing computations on platform Host. Devices:\n",
      "2025-07-29 13:08:59.056773: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2025-07-29 13:08:59.090019: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x741f54859e90>,\n",
       " <keras.layers.core.Dense at 0x741f4d95de10>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x741fc2d8e710>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x741fc2dd3490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc2dd3a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc2d8e310>,\n",
       " <keras.layers.core.Activation at 0x741fc2d8e450>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x741fc2dd3d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc2ccd890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc2c52a50>,\n",
       " <keras.layers.core.Activation at 0x741fc2c52c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc076ce50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc06f8990>,\n",
       " <keras.layers.core.Activation at 0x741fc06f88d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc066d6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc0508350>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc05e8290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc04ea790>,\n",
       " <keras.layers.merge.Add at 0x741fc046d590>,\n",
       " <keras.layers.core.Activation at 0x741fc0480310>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc0480410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc03fef90>,\n",
       " <keras.layers.core.Activation at 0x741fc03fed90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc031f250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc0428d50>,\n",
       " <keras.layers.core.Activation at 0x741fc02fcc90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc021c050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc0195110>,\n",
       " <keras.layers.merge.Add at 0x741fc01952d0>,\n",
       " <keras.layers.core.Activation at 0x741fc01326d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc0132a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fc00a9c90>,\n",
       " <keras.layers.core.Activation at 0x741fc00a97d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fb07bcd90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fb0797710>,\n",
       " <keras.layers.core.Activation at 0x741fb0797f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fb0737450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fb06b0090>,\n",
       " <keras.layers.merge.Add at 0x741fb06b0250>,\n",
       " <keras.layers.core.Activation at 0x741fb05ce210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fb0562090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fb05b1a90>,\n",
       " <keras.layers.core.Activation at 0x741fb0547d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fb04e65d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fb0471210>,\n",
       " <keras.layers.core.Activation at 0x741fb04487d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fc2f864d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fb02d16d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fb03b3d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fb02f4a10>,\n",
       " <keras.layers.merge.Add at 0x741fb0228a50>,\n",
       " <keras.layers.core.Activation at 0x741fb01d5890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fb01d5790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fb0151250>,\n",
       " <keras.layers.core.Activation at 0x741fb0151050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741fb00eba10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741fb0065ed0>,\n",
       " <keras.layers.core.Activation at 0x741f907c8050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f90742e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f906ce190>,\n",
       " <keras.layers.merge.Add at 0x741f9071f690>,\n",
       " <keras.layers.core.Activation at 0x741f9063fe50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f9063fa10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f905bd250>,\n",
       " <keras.layers.core.Activation at 0x741f905bd050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f90556e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f904d0f50>,\n",
       " <keras.layers.core.Activation at 0x741f904eb250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f9046fd10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f9038c410>,\n",
       " <keras.layers.merge.Add at 0x741f903d3f50>,\n",
       " <keras.layers.core.Activation at 0x741f9036bc10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f90358fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f902eb3d0>,\n",
       " <keras.layers.core.Activation at 0x741f902eb250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f90203990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f90179e50>,\n",
       " <keras.layers.core.Activation at 0x741f9019fe90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f901199d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f9007b690>,\n",
       " <keras.layers.merge.Add at 0x741f9007b790>,\n",
       " <keras.layers.core.Activation at 0x741f787d6e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f787d6e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f78755b90>,\n",
       " <keras.layers.core.Activation at 0x741f78755090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f786f1850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f78651f90>,\n",
       " <keras.layers.core.Activation at 0x741f785f29d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f78588e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f78481b10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f78568650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f7843a710>,\n",
       " <keras.layers.merge.Add at 0x741f78385590>,\n",
       " <keras.layers.core.Activation at 0x741f78398310>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f78398590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f78318a90>,\n",
       " <keras.layers.core.Activation at 0x741f78318dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f782b5e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f7821de50>,\n",
       " <keras.layers.core.Activation at 0x741f78232e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f7814b110>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f78130ad0>,\n",
       " <keras.layers.merge.Add at 0x741f78130410>,\n",
       " <keras.layers.core.Activation at 0x741f780a1d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f7804c4d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f55f88350>,\n",
       " <keras.layers.core.Activation at 0x741f55f88450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f55ec2c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f55e5c410>,\n",
       " <keras.layers.core.Activation at 0x741f55ea2f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f55e3a390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f55d5c290>,\n",
       " <keras.layers.merge.Add at 0x741f55db3110>,\n",
       " <keras.layers.core.Activation at 0x741f55cd3210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f55e3ab90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f55cb6950>,\n",
       " <keras.layers.core.Activation at 0x741f55c4be90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f55beaed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f55b63f10>,\n",
       " <keras.layers.core.Activation at 0x741f55b2f490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f55a80c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f55a1c450>,\n",
       " <keras.layers.merge.Add at 0x741f55a65590>,\n",
       " <keras.layers.core.Activation at 0x741f559faa90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f559a1e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f5597bed0>,\n",
       " <keras.layers.core.Activation at 0x741f558956d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f558b9e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f5580f2d0>,\n",
       " <keras.layers.core.Activation at 0x741f5580f410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f557a7a10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f55724fd0>,\n",
       " <keras.layers.merge.Add at 0x741f556c6c10>,\n",
       " <keras.layers.core.Activation at 0x741f55645e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f5565f850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f55628750>,\n",
       " <keras.layers.core.Activation at 0x741f55628b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f55558410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f554bcb90>,\n",
       " <keras.layers.core.Activation at 0x741f554bc4d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f5545b790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f553d1fd0>,\n",
       " <keras.layers.merge.Add at 0x741f553d1490>,\n",
       " <keras.layers.core.Activation at 0x741f5536df50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f55313ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f552ecf10>,\n",
       " <keras.layers.core.Activation at 0x741f552ecbd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f55205ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f551ebb50>,\n",
       " <keras.layers.core.Activation at 0x741f551ebf50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f5511b910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f5501dfd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f550b77d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f54f93590>,\n",
       " <keras.layers.merge.Add at 0x741f54fce590>,\n",
       " <keras.layers.core.Activation at 0x741f54f36d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f54f36850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f54eac990>,\n",
       " <keras.layers.core.Activation at 0x741f54eac8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f54df2a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f54d465d0>,\n",
       " <keras.layers.core.Activation at 0x741f54d466d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f54cdf890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f54c44f90>,\n",
       " <keras.layers.merge.Add at 0x741f54c44e10>,\n",
       " <keras.layers.core.Activation at 0x741f54bfa610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f54b97790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f54b5bf50>,\n",
       " <keras.layers.core.Activation at 0x741f54b75d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f54a8e850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f549d4ed0>,\n",
       " <keras.layers.core.Activation at 0x741f54991ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x741f549a7ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x741f54931e10>,\n",
       " <keras.layers.merge.Add at 0x741f5490a4d0>,\n",
       " <keras.layers.core.Activation at 0x741f548a3850>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x741f54bfa690>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x741fc3828690>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      " 11/101 [==>...........................] - ETA: 1:35:07 - loss: 0.3823 - acc: 0.8345"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
