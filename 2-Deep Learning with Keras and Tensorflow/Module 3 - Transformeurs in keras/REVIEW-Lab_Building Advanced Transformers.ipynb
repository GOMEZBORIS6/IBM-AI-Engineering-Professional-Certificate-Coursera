{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand the core components of a Transformer architecture.\n",
    "- Implement a multi-head self-attention mechanism from scratch.\n",
    "- Train and evaluate a Transformer for time series prediction.\n",
    "- Handle preprocessing and scaling for time series data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Transformer?\n",
    "\n",
    "The Transformer architecture was introduced in the paper *\"Attention Is All You Need\"*. It revolutionized natural language processing by using attention mechanisms instead of recurrence.\n",
    "\n",
    "### Key Components:\n",
    "- **Input Embedding:** Converts input tokens (or time steps) into vectors.\n",
    "- **Positional Encoding:** Injects information about the position of input tokens.\n",
    "- **Multi-Head Self-Attention:** Allows the model to focus on different parts of the input sequence.\n",
    "- **Feedforward Layers:** Process the attended information.\n",
    "- **Layer Normalization & Residual Connections:** Stabilize and speed up training.\n",
    "\n",
    "> Transformers are now widely used not only in NLP but also in time series forecasting, image recognition, and more.\n",
    "\n",
    "**Next:** You will implement parts of this architecture step-by-step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.73.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, pyarrow, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 h5py-3.14.0 keras-3.10.0 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.1.3 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 pyarrow-20.0.0 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.3.1 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.1.3)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m162.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.1/35.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.5-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (106 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m158.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.5-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m152.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m159.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.5 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.3.0 pyparsing-3.2.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 08:08:57.728800: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-15 08:08:57.730121: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-15 08:08:57.734664: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-15 08:08:57.747382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752566937.767976     301 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752566937.773848     301 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752566937.790567     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752566937.790599     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752566937.790601     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752566937.790603     301 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-15 08:08:57.797919: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 08:10:46.178683: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - loss: 11.3242 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.2303\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1662\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1334 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1237 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1499 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1041 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1005 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1137   \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1327 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.1428 \n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0689 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0682 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.1021 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0766 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0495 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0455 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0482 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0428 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76a2bb415520>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 345ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    "\n",
    "# Prepare true values for comparison\n",
    "true_values = scaler.inverse_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Plot the predictions vs true values\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(true_values, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.title('Predictions vs True Data (Both Scaled Back)')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - loss: 6.1199  \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 1.5482 \n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.6697 \n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.2947 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.1238 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0832 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - loss: 0.0599 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1s/step - loss: 0.0514 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0437 \n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 1s/step - loss: 0.0436 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0350 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0328 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0310 \n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0273 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1s/step - loss: 0.0237 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0207 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0258 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0230\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 304ms/step - loss: 0.0015\n",
      "Test loss: 0.002518082968890667\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "  \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "\n",
    "dropout = Dropout(0.5)(flatten) \n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "  \n",
    "\n",
    "# Build the model \n",
    "\n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "  \n",
    "\n",
    "# Compile the model \n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "  \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32) \n",
    "\n",
    "  \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "\n",
    "print(f'Test loss: {loss}') \n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 613ms/step - loss: 0.0351 \n",
      "Epoch 2/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 609ms/step - loss: 0.0384 \n",
      "Epoch 3/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 605ms/step - loss: 0.0308 \n",
      "Epoch 4/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 608ms/step - loss: 0.0358 \n",
      "Epoch 5/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 613ms/step - loss: 0.0212 \n",
      "Epoch 6/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 616ms/step - loss: 0.0216 \n",
      "Epoch 7/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 613ms/step - loss: 0.0310 \n",
      "Epoch 8/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 613ms/step - loss: 0.0288 \n",
      "Epoch 9/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 622ms/step - loss: 0.0166 \n",
      "Epoch 10/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 618ms/step - loss: 0.0201 \n",
      "Epoch 11/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 614ms/step - loss: 0.0117 \n",
      "Epoch 12/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 610ms/step - loss: 0.0115 \n",
      "Epoch 13/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 617ms/step - loss: 0.0092 \n",
      "Epoch 14/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 619ms/step - loss: 0.0140 \n",
      "Epoch 15/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 618ms/step - loss: 0.0152 \n",
      "Epoch 16/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 618ms/step - loss: 0.0124 \n",
      "Epoch 17/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 618ms/step - loss: 0.0143 \n",
      "Epoch 18/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 618ms/step - loss: 0.0117 \n",
      "Epoch 19/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 622ms/step - loss: 0.0113 \n",
      "Epoch 20/20\n",
      "\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 621ms/step - loss: 0.0128 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 314ms/step - loss: 0.0069\n",
      "Test loss with batch size 16: 0.01134856790304184\n",
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0099 \n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0036\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0031\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0028\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0029\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0030\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0035\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0026\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0024\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0024\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0027\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0022\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0022\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - loss: 0.0022\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0024\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0020\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - loss: 0.0026\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0023\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0019\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 2s/step - loss: 0.0020\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 303ms/step - loss: 0.0037\n",
      "Test loss with batch size 64: 0.002070715883746743\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Train the model with batch size 16\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - loss: 0.2296 \n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0187\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0047\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0043 \n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0027 \n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0025 \n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0022 \n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0014 \n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0017\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0018 \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0014\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0043 \n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0019 \n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0012 \n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0019\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0016 \n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - loss: 0.0015 \n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0022 \n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0042 \n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 1s/step - loss: 0.0027 \n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 310ms/step - loss: 0.0044\n",
      "Test loss with tanh activation: 0.002109814900904894\n"
     ]
    }
   ],
   "source": [
    "## Write your code here.\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click here to view the solution.</summary>\n",
    "\n",
    "```\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8aae4de69f29de06e63c5f2d04ef24811d42d1553c8ac316f7ad75d55f2c2d79"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
